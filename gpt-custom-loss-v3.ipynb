{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9952312,"sourceType":"datasetVersion","datasetId":6109615},{"sourceId":182861,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":155865,"modelId":178321},{"sourceId":185768,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":158369,"modelId":180735},{"sourceId":187583,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":159914,"modelId":182290}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try:\n  import torchsummary\nexcept:\n  !pip install torchsummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:49:49.238597Z","iopub.execute_input":"2024-12-04T01:49:49.238863Z","iopub.status.idle":"2024-12-04T01:49:58.640503Z","shell.execute_reply.started":"2024-12-04T01:49:49.238835Z","shell.execute_reply":"2024-12-04T01:49:58.639374Z"}},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport re\nimport torch\nfrom torch import nn\nfrom termcolor import colored \nfrom torchsummary import summary\nfrom transformers import GPT2Tokenizer\nfrom tokenizers import ByteLevelBPETokenizer\nfrom torch.nn.utils import clip_grad_norm_\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:49:58.642265Z","iopub.execute_input":"2024-12-04T01:49:58.642553Z","iopub.status.idle":"2024-12-04T01:50:03.003411Z","shell.execute_reply.started":"2024-12-04T01:49:58.642524Z","shell.execute_reply":"2024-12-04T01:50:03.002487Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gpt2_model/pytorch/default/1/gpt2_model (1).pth\n/kaggle/input/model_gpt/pytorch/default/1/gpt_2_custom_loss_v3.pth.tar\n/kaggle/input/poet-datadet-2/new_data_clean2.csv\n/kaggle/input/poet-datadet-2/start_vowels.txt\n/kaggle/input/poet-datadet-2/tone_dict.txt\n/kaggle/input/poet-datadet-2/rhymes.txt\n/kaggle/input/gpt_model_custom_loss/pytorch/default/1/gpt_2_custom_loss_v2.pth.tar\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/poet-datadet-2/new_data_clean2.csv')\ndata.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:16.722409Z","iopub.execute_input":"2024-12-04T01:50:16.723155Z","iopub.status.idle":"2024-12-04T01:50:18.096283Z","shell.execute_reply.started":"2024-12-04T01:50:16.723118Z","shell.execute_reply":"2024-12-04T01:50:18.095367Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(255080, 2)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"sentences = data.content.values\nsentences[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:19.412486Z","iopub.execute_input":"2024-12-04T01:50:19.413156Z","iopub.status.idle":"2024-12-04T01:50:19.421611Z","shell.execute_reply.started":"2024-12-04T01:50:19.413122Z","shell.execute_reply":"2024-12-04T01:50:19.420819Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'ai về xa mãi cô thôn\\nmột mình trông khói hoàng hôn nhớ nhà\\nngày em mới bước chân ra\\ntuy rằng cách mặt lòng ta chưa sầu'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def replace_context(text):\n    text = text.split('\\n')\n    s = ''\n    for i in text:\n        s = s + i + ' \\\\n '\n    return s[:-4]\n\nreplace_context('thăm con ở trại nhi đồng\\nmột ngày xuân đẹp nắng hồng thướt tha\\ncon đang cùng bạn múa ca\\ncành tơ phơ phất gió qua rì rào')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:19.969246Z","iopub.execute_input":"2024-12-04T01:50:19.969513Z","iopub.status.idle":"2024-12-04T01:50:19.975661Z","shell.execute_reply.started":"2024-12-04T01:50:19.969489Z","shell.execute_reply":"2024-12-04T01:50:19.974891Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'thăm con ở trại nhi đồng \\\\n một ngày xuân đẹp nắng hồng thướt tha \\\\n con đang cùng bạn múa ca \\\\n cành tơ phơ phất gió qua rì rào'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"sentences = [replace_context(x) for x in sentences]\nsentences[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:20.638634Z","iopub.execute_input":"2024-12-04T01:50:20.639274Z","iopub.status.idle":"2024-12-04T01:50:21.039783Z","shell.execute_reply.started":"2024-12-04T01:50:20.639244Z","shell.execute_reply":"2024-12-04T01:50:21.038934Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'thăm con ở trại nhi đồng \\\\n một ngày xuân đẹp nắng hồng thướt tha \\\\n con đang cùng bạn múa ca \\\\n cành tơ phơ phất gió qua rì rào'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def add_token(text):\n    return '<s> ' + text + ' </s>'\nsentences = [add_token(x) for x in sentences]\nsentences[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:21.308141Z","iopub.execute_input":"2024-12-04T01:50:21.308407Z","iopub.status.idle":"2024-12-04T01:50:21.460524Z","shell.execute_reply.started":"2024-12-04T01:50:21.308383Z","shell.execute_reply":"2024-12-04T01:50:21.459708Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'<s> thăm con ở trại nhi đồng \\\\n một ngày xuân đẹp nắng hồng thướt tha \\\\n con đang cùng bạn múa ca \\\\n cành tơ phơ phất gió qua rì rào </s>'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"with open('data.txt', 'w', encoding='utf-8') as file:\n    for sentence in sentences:\n        file.write(sentence + '\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:21.938561Z","iopub.execute_input":"2024-12-04T01:50:21.938829Z","iopub.status.idle":"2024-12-04T01:50:22.185791Z","shell.execute_reply.started":"2024-12-04T01:50:21.938805Z","shell.execute_reply":"2024-12-04T01:50:22.184892Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def read_text_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        sentences = file.readlines()\n    return sentences\n\nfile_path = '/kaggle/working/data.txt'\n\nsentences = read_text_file(file_path)\n\nfor i, sentence in enumerate(sentences[:5]):\n    print(f\"Summary {i+1}: {sentence.strip()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:22.647437Z","iopub.execute_input":"2024-12-04T01:50:22.647735Z","iopub.status.idle":"2024-12-04T01:50:22.985927Z","shell.execute_reply.started":"2024-12-04T01:50:22.647707Z","shell.execute_reply":"2024-12-04T01:50:22.985022Z"}},"outputs":[{"name":"stdout","text":"Summary 1: <s> thăm con ở trại nhi đồng \\n một ngày xuân đẹp nắng hồng thướt tha \\n con đang cùng bạn múa ca \\n cành tơ phơ phất gió qua rì rào </s>\nSummary 2: <s> con đang cùng bạn múa ca \\n cành tơ phơ phất gió qua rì rào \\n tiếng ca bay lượn từng cao \\n trăm con chim nhỏ ngọt ngào không gian </s>\nSummary 3: <s> ai về xa mãi cô thôn \\n một mình trông khói hoàng hôn nhớ nhà \\n ngày em mới bước chân ra \\n tuy rằng cách mặt lòng ta chưa sầu </s>\nSummary 4: <s> ngày em mới bước chân ra \\n tuy rằng cách mặt lòng ta chưa sầu \\n nắng trôi vàng chẩy về đâu \\n hôm nay mới thực bắt đầu vào thu </s>\nSummary 5: <s> trời hồng chắc má em tươi \\n nước trong chắc miệng em cười thêm xinh \\n em đi hoài cảm một mình \\n hai lòng riêng để mối tình cô đơn </s>\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"sentences = [x.strip() for x in sentences]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:23.238326Z","iopub.execute_input":"2024-12-04T01:50:23.238616Z","iopub.status.idle":"2024-12-04T01:50:23.353796Z","shell.execute_reply.started":"2024-12-04T01:50:23.238588Z","shell.execute_reply":"2024-12-04T01:50:23.352908Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import ast\nfrom math import ceil, floor\n\n# try:\n#     from importlib import resources\n# except ImportError:\n#     import importlib_resources as resources\n\n\ndef load_data(filename: str):\n\n    with open(filename,'r', encoding='utf8') as file:\n        text = file.read()\n\n    content = ast.literal_eval(text)\n    return content\n\n\nvowels_path = \"/kaggle/input/poet-datadet-2/start_vowels.txt\"\nstart_vowels = load_data(vowels_path)\n\nhuyen = start_vowels['huyen']\nsac = start_vowels['sac']\nnang = start_vowels['nang']\nhoi = start_vowels['hoi']\nnga = start_vowels['nga']\nkhong_dau = start_vowels['khong_dau']\n\nlist_start_vowels = []\nlist_start_vowels.extend(huyen)\nlist_start_vowels.extend(sac)\nlist_start_vowels.extend(nang)\nlist_start_vowels.extend(hoi)\nlist_start_vowels.extend(nga)\nlist_start_vowels.extend(khong_dau)\n\nrhyme_path = \"/kaggle/input/poet-datadet-2/rhymes.txt\"\n\nrhymes_dict = load_data(rhyme_path)\n\n\neven_chars = []\n\neven_chars.extend(huyen)\neven_chars.extend(khong_dau)\n\ntone_dict = load_data(\"/kaggle/input/poet-datadet-2/tone_dict.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:24.057557Z","iopub.execute_input":"2024-12-04T01:50:24.057874Z","iopub.status.idle":"2024-12-04T01:50:24.107481Z","shell.execute_reply.started":"2024-12-04T01:50:24.057845Z","shell.execute_reply":"2024-12-04T01:50:24.106565Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def is_stanza(sentences: str):\n    \"\"\"\n    Check if input is a stanza or not\n\n    param sentences: sentences to check\n\n    return: is stanza or not\n    \"\"\"\n    return len(sentences.split(\"\\n\\n\")) == 1\n\n\ndef split_word(word):\n    \"\"\"\n        Split word by 2 part, starting and ending\n\n        param word: word to split\n\n        return: ending part of word\n        Ex: mùa -> ùa\n    \"\"\"\n    word_length = len(word)\n    start_index = 0\n    prev = ''\n    for i in range(word_length):\n        if prev == 'g' and word[i] == 'i':\n            continue\n        if prev == 'q' and word[i] == 'u':\n            continue\n        if word[i] in list_start_vowels:\n            start_index = i\n            break\n        prev = word[i]\n    return word[start_index:]\n\n\ndef compare(word1: str, word2: str):\n    \"\"\"\n    Check 2 words rhyme if the same\n    \n    param word1, word2: words to check\n    \n    return: is the same rhyme or not\n    \"\"\"\n    rhyme1 = split_word(word1)\n    rhyme2 = split_word(word2)\n\n    if rhyme2 in rhymes_dict[rhyme1]:\n        return True\n    return False\n\n\ndef check_rhyme_pair(prev_sentence: str, cur_sentence: str, prev_eight_words_rhyme=\"\"):\n    \"\"\"\n        Check 2 words rhyme if the same\n\n        param word1, word2: words to check\n\n        return: is the same rhyme or not\n    \"\"\"\n    rhyme_errors = 0\n    length_errors = 0\n\n    prev_length = len(prev_sentence.split(\" \"))\n    cur_length = len(cur_sentence.split(\" \"))\n    s = ''\n\n    if prev_length != 6:\n        prev_sentence = \"(L)\" + prev_sentence\n        length_errors = length_errors + 1\n\n    if cur_length != 8:\n        cur_sentence = \"(L)\" + cur_sentence\n        length_errors = length_errors + 1\n\n    prev_words = prev_sentence.split(\" \")\n    cur_words = cur_sentence.split(\" \")\n\n    if prev_eight_words_rhyme == \"\":\n        try:\n            if not compare(prev_words[5], cur_words[5]):\n                cur_words[5] = cur_words[5] + \"(V)\"\n                rhyme_errors = rhyme_errors + 1\n        except Exception as e:\n            s = f\"{e} + {cur_sentence}\"\n            pass\n    if prev_eight_words_rhyme != \"\":\n        try:\n            if not compare(prev_words[5], prev_eight_words_rhyme):\n                prev_words[5] = prev_words[5] + \"(V)\"\n                rhyme_errors = rhyme_errors + 1\n        except Exception as e:\n            s = f\"{e} + {cur_sentence}\"\n            pass\n        try:\n            if not compare(prev_eight_words_rhyme, cur_words[5]):\n                cur_words[5] = cur_words[5] + \"(V)\"\n                rhyme_errors = rhyme_errors + 1\n        except Exception as e:\n            s = f\"{e} + {cur_sentence}\"\n            pass\n    prev_sentence =  \" \".join(prev_words)\n    cur_sentence =  \" \".join(cur_words)\n\n    return prev_sentence, cur_sentence, cur_words[-1], rhyme_errors, length_errors, s\n\ndef preprocess_stanza(stanza: str):\n    \"\"\"\n    A function to process Stanza to remove all unnecessary blank\n\n    param sentence: stanza to process\n\n    return: stanza processed\n    \"\"\"\n    sentences = stanza.split(\"\\\\n\")\n    sentences_out = []\n    for sentence in sentences:\n        words = sentence.split(\" \")\n        words_out = []\n        for word in words:\n            if word:\n                words_out.append(word)\n        sentences_out.append(\" \".join(words_out))\n    return \"\\\\n\".join(sentences_out)\n    \ndef check_rhyme_stanza(stanza: str):\n    \"\"\"\n        Check rhyme by stanza\n\n        param stanza: input stanza to check\n\n        return: res: stanza after check filter and error highlighted\n                total_rhyme_errors: total rhyme errors\n                total_length_errors: total length errors\n    \"\"\"\n    sentences = stanza.split(\"\\\\n\")\n    first_words = sentences[0].split(\" \")\n    start_index = 0\n    prev_eight_words_rhyme = \"\"\n    total_rhyme_errors = 0\n    total_length_errors = 0\n\n    if len(first_words) == 8:\n        prev_eight_words_rhyme = split_word(first_words[7])\n        start_index = 1\n\n    for i in range(start_index, len(sentences), 2):\n        if i+1 == len(sentences):\n            sentences.append(\"Missing ending sentence\")\n        sentences[i], sentences[i+1], prev_eight_words_rhyme, rhyme_errors, length_errors, s = check_rhyme_pair(sentences[i], sentences[i + 1], prev_eight_words_rhyme)\n    \n        total_rhyme_errors = total_rhyme_errors + rhyme_errors + len(s)\n        total_length_errors = total_length_errors + length_errors + len(s)\n    res = \"\\\\n\".join(sentences)\n    return res, total_rhyme_errors, total_length_errors\n\n\ndef extract_consonants(word):\n    # Danh sách các nguyên âm tiếng Việt\n    consonants = [char for char in word if char.lower() in list_start_vowels]\n    return consonants\n\ndef get_tone(word: str):\n    \"\"\"\n        Check word is even tone or not\n\n        param word: word to check tone\n\n        return: even or uneven\n    \"\"\"\n    char = split_word(word)\n    chars = extract_consonants(char)\n    for char in chars:\n        if char not in even_chars:\n            return 'uneven'\n    return 'even'\n\ndef check_tone_sentence(sentence: str):\n    \"\"\"\n        Check sentence is on the right form of even or uneven rule\n\n        param sentence: sentence to check tone\n\n        return: sentences after added notation to highlight error\n                total_wrong_tone: total wrong tone in sentence\n    \"\"\"\n    words = sentence.split(\" \")\n    length = len(words)\n    if length != 6 and length != 8:\n        return \"(L)\"+sentence, 0\n    cur_tone_dict = tone_dict[length]\n    total_wrong_tone = 0\n    for i in cur_tone_dict:\n        if get_tone(words[i]) != cur_tone_dict[i]:\n            total_wrong_tone = total_wrong_tone + 1\n            words[i] = words[i] + \"(T)\"\n    return \" \".join(words), total_wrong_tone\n\ndef check_tone_stanza(stanza: str):\n    \"\"\"\n        Check stanza is on the right form of even or uneven rule\n\n        param sentence: stanza to check tone\n\n        return: stanza after added notation to highlight error\n                total_wrong_tone: total wrong tone in sentence\n    \"\"\"\n    sentences = stanza.split(\"\\\\n\")\n    total_wrong = 0\n    for i in range(len(sentences)):\n        current_wrong = 0\n        sentences[i], current_wrong = check_tone_sentence(sentences[i])\n        total_wrong = total_wrong + current_wrong\n    return \"\\\\n\".join(sentences), total_wrong\n\ndef check_rule(stanza: str):\n    \"\"\"\n    A function to check both rhyme and tone rule\n\n    param sentence: stanza to check\n\n    return: stanza processed\n    \"\"\"\n    if not is_stanza(stanza):\n        print(stanza + \": is not a stanza\")\n        return\n    stanza = preprocess_stanza(stanza)\n    stanza, total_rhyme_errors, total_length_errors = check_rhyme_stanza(stanza)\n    stanza, total_wrong_tone = check_tone_stanza(stanza)\n    return stanza, total_length_errors, total_rhyme_errors, total_wrong_tone\n\n\n\ndef calculate_score_by_error(stanza_length: int, total_length_errors=0, total_rhyme_errors=0, total_wrong_tone=0):\n    \"\"\"\n      A function to calculate score for the Stanza by length, rhyme and tone errors\n          Currently doesnt punish the length error\n\n      param sentence: stanza_length,\n                      total_length_errors,\n                      total_rhyme_errors,\n                      total_wrong_tone\n\n      return: score calculated by formula that rhyme accounts for 70% score rate and 30% left for tone\n    \"\"\"\n    try:\n        num_six = ceil(stanza_length/2)\n        num_eight = floor(stanza_length/2)\n    \n        rhyme_minus_points = 70*total_rhyme_errors/(num_six + 2*num_eight-1)\n        tone_minus_points = 30*total_wrong_tone/(3*num_six+4*num_eight)\n        \n    \n        return rhyme_minus_points + tone_minus_points + (total_length_errors * 5)\n    except:\n        return 50\n\n\ndef calculate_stanza_score(stanza: str):\n   \"\"\"\n      A function to calculate score for the Stanza\n\n      param sentence: stanza\n\n      return: score  after checked by rule and calculated by formula that rhyme accounts for 70% score rate\n      and 30% left for tone\n   \"\"\"\n   stanza = preprocess_stanza(stanza)\n   length = len(stanza.split(\"\\\\n\"))\n   stanza, total_length_errors, total_rhyme_errors, total_wrong_tone = check_rule(stanza)\n   score = calculate_score_by_error(length, total_length_errors, total_rhyme_errors, total_wrong_tone)\n\n   return score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:31.094106Z","iopub.execute_input":"2024-12-04T01:50:31.094428Z","iopub.status.idle":"2024-12-04T01:50:31.116266Z","shell.execute_reply.started":"2024-12-04T01:50:31.094399Z","shell.execute_reply":"2024-12-04T01:50:31.115306Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"tokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files=[\"/kaggle/working/data.txt\"], min_frequency=2, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:31.791196Z","iopub.execute_input":"2024-12-04T01:50:31.791500Z","iopub.status.idle":"2024-12-04T01:50:39.029859Z","shell.execute_reply.started":"2024-12-04T01:50:31.791472Z","shell.execute_reply":"2024-12-04T01:50:39.028823Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"tokenizer.save_model(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:39.031513Z","iopub.execute_input":"2024-12-04T01:50:39.031824Z","iopub.status.idle":"2024-12-04T01:50:39.041654Z","shell.execute_reply.started":"2024-12-04T01:50:39.031794Z","shell.execute_reply":"2024-12-04T01:50:39.040700Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/vocab.json', '/kaggle/working/merges.txt']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def tokenize_function(text):\n    encoded = tokenizer.encode(text)\n    input_ids = torch.tensor(encoded.ids).unsqueeze(0)\n    attention_mask = torch.tensor(encoded.attention_mask).unsqueeze(0)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:39.042945Z","iopub.execute_input":"2024-12-04T01:50:39.043584Z","iopub.status.idle":"2024-12-04T01:50:39.052635Z","shell.execute_reply.started":"2024-12-04T01:50:39.043544Z","shell.execute_reply":"2024-12-04T01:50:39.051839Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenized_data = [tokenize_function(sample) for sample in sentences]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:50:39.551076Z","iopub.execute_input":"2024-12-04T01:50:39.551349Z","iopub.status.idle":"2024-12-04T01:51:19.527285Z","shell.execute_reply.started":"2024-12-04T01:50:39.551324Z","shell.execute_reply":"2024-12-04T01:51:19.526534Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"vocab_size = len(tokenizer.get_vocab()) \n\nprint(colored(f\"Original vocabulary size: {vocab_size}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:51:19.528834Z","iopub.execute_input":"2024-12-04T01:51:19.529116Z","iopub.status.idle":"2024-12-04T01:51:19.542187Z","shell.execute_reply.started":"2024-12-04T01:51:19.529090Z","shell.execute_reply":"2024-12-04T01:51:19.541374Z"}},"outputs":[{"name":"stdout","text":"Original vocabulary size: 9138\u001b[0m\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"max_len = [len(x['input_ids'][0]) for x in tokenized_data]\nmax_len = max(max_len)\nmax_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:51:19.543464Z","iopub.execute_input":"2024-12-04T01:51:19.544181Z","iopub.status.idle":"2024-12-04T01:51:20.476043Z","shell.execute_reply.started":"2024-12-04T01:51:19.544142Z","shell.execute_reply":"2024-12-04T01:51:20.475223Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"45"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nclass TextDataset(Dataset):\n  def __init__(self, tokenized_data, max_len): \n    self.data = tokenized_data\n    self.max_len = max_len\n\n  def __len__(self):\n    return len(self.data)\n\n  def __getitem__(self, idx):\n    sample = self.data[idx]\n    # Pad input_ids and attention_mask to the same length (max_len)\n    input_ids = sample['input_ids']\n    attention_mask = sample['attention_mask']\n    padded_input_ids = torch.nn.functional.pad(input_ids, (0, self.max_len - input_ids.shape[1]), value=0)\n    padded_attention_mask = torch.nn.functional.pad(attention_mask, (0, self.max_len - attention_mask.shape[1]), value=0)\n    return {\n        'input_ids': padded_input_ids,\n        'attention_mask': padded_attention_mask\n    }   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:52:30.918671Z","iopub.execute_input":"2024-12-04T01:52:30.919400Z","iopub.status.idle":"2024-12-04T01:52:30.925273Z","shell.execute_reply.started":"2024-12-04T01:52:30.919367Z","shell.execute_reply":"2024-12-04T01:52:30.924324Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"dataset = TextDataset(tokenized_data, max_len)  \ndataloader = DataLoader(dataset, batch_size=5, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:52:31.729068Z","iopub.execute_input":"2024-12-04T01:52:31.729737Z","iopub.status.idle":"2024-12-04T01:52:31.733803Z","shell.execute_reply.started":"2024-12-04T01:52:31.729706Z","shell.execute_reply":"2024-12-04T01:52:31.732852Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class GPT2Model(nn.Module):\n    def __init__(self, vocab_size, n_positions=1024, n_ctx=1024, n_embd=768, n_layer=12, n_head=12):\n        super(GPT2Model, self).__init__()\n        self.wte = nn.Embedding(vocab_size, n_embd)\n        self.wpe = nn.Embedding(n_positions, n_embd)\n        self.h = nn.ModuleList([nn.TransformerEncoderLayer(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n\n    def forward(self, input_ids, position_ids=None):\n        if position_ids is None:\n            position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        inputs_embeds = self.wte(input_ids) + self.wpe(position_ids)\n        hidden_states = inputs_embeds\n        for block in self.h:\n            hidden_states = block(hidden_states)\n        hidden_states = self.ln_f(hidden_states)\n        logits = self.head(hidden_states)\n        return logits, hidden_states\n    def get_num_parameters(self):  # Define function inside the class\n        total_params = 0\n        for name, param in self.named_parameters():\n          if param.requires_grad:\n            total_params += param.numel()\n        return total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:52:34.536204Z","iopub.execute_input":"2024-12-04T01:52:34.536511Z","iopub.status.idle":"2024-12-04T01:52:34.544378Z","shell.execute_reply.started":"2024-12-04T01:52:34.536486Z","shell.execute_reply":"2024-12-04T01:52:34.543465Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ngpt_model = GPT2Model(vocab_size)\n\n# gpt_model.load_state_dict(torch.load('/kaggle/input/gpt2_model/pytorch/default/1/gpt2_model (1).pth'))\ngpt_model.to(device)\n# gpt_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:52:37.475506Z","iopub.execute_input":"2024-12-04T01:52:37.476358Z","iopub.status.idle":"2024-12-04T01:52:38.580386Z","shell.execute_reply.started":"2024-12-04T01:52:37.476324Z","shell.execute_reply":"2024-12-04T01:52:38.579531Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"GPT2Model(\n  (wte): Embedding(9138, 768)\n  (wpe): Embedding(1024, 768)\n  (h): ModuleList(\n    (0-11): 12 x TransformerEncoderLayer(\n      (self_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=768, out_features=9138, bias=False)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:52:42.072371Z","iopub.execute_input":"2024-12-04T01:52:42.072715Z","iopub.status.idle":"2024-12-04T01:52:42.077230Z","shell.execute_reply.started":"2024-12-04T01:52:42.072686Z","shell.execute_reply":"2024-12-04T01:52:42.076174Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def save_checkpoint(state, filename= \"GPT-2/gpt_2_custom_loss_v2.pth.tar\"):\n    print(\"Saving checkpoint\")\n    torch.save(state,filename)\n\ndef load_checkpoint(state):\n    print(\"Load checkpoint\")\n    gpt_model.load_state_dict(state['state_dict'])\n    optimizer.load_state_dict(state['optimizer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:52:46.508642Z","iopub.execute_input":"2024-12-04T01:52:46.509032Z","iopub.status.idle":"2024-12-04T01:52:46.513849Z","shell.execute_reply.started":"2024-12-04T01:52:46.508997Z","shell.execute_reply":"2024-12-04T01:52:46.513040Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"optimizer = optim.Adam(gpt_model.parameters(), lr=5e-5)\nload_checkpoint(torch.load('/kaggle/input/model_gpt/pytorch/default/1/gpt_2_custom_loss_v3.pth.tar'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:53:08.255478Z","iopub.execute_input":"2024-12-04T01:53:08.255823Z","iopub.status.idle":"2024-12-04T01:53:14.273827Z","shell.execute_reply.started":"2024-12-04T01:53:08.255793Z","shell.execute_reply":"2024-12-04T01:53:14.272895Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/711693160.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  load_checkpoint(torch.load('/kaggle/input/model_gpt/pytorch/default/1/gpt_2_custom_loss_v3.pth.tar'))\n","output_type":"stream"},{"name":"stdout","text":"Load checkpoint\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7, top_k=50, device='cuda'):\n\n    model.eval()  # Set model to evaluation mode\n    \n    # Tokenize the prompt\n    input_ids = torch.tensor(tokenizer.encode(prompt).ids)\n    input_ids = input_ids.unsqueeze(0) \n    input_ids = input_ids.to(device)\n    \n    # Create attention mask\n    # attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n    generated = input_ids[0].tolist()  # Include the prompt tokens\n    current_length = len(generated)\n    \n    with torch.no_grad():\n        while current_length < max_length:\n            # Get model output\n            outputs = model(input_ids)\n            next_token_logits = outputs[0][0, -1, :] / temperature  # Take last token prediction\n            \n            # Apply top-k filtering\n            if top_k > 0:\n                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n                top_k_tokens = [tokenizer.decode([idx.item()]) for idx in top_k_indices]\n                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n                next_token_logits[top_k_indices] = top_k_logits\n            \n            # Sample from the filtered distribution\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            \n            # Append the next token to input_ids\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n            generated.append(next_token.item())\n            current_length += 1\n            \n            # Try to decode the current sequence\n            try:\n                current_text = tokenizer.decode(generated)\n                # If we get a natural break point (e.g., end of sentence), we can stop\n                if current_text.endswith(('.', '!', '?', '\\n')) and current_length > len(encoded.ids):\n                    break\n            except:\n                continue\n    \n    try:\n        generated_text = tokenizer.decode(generated)\n    except:\n        # If decoding fails, return what we have up to the last successful token\n        generated_text = tokenizer.decode(generated[:-1])\n    \n    return generated_text\n\n\nprompt = \"<s> thăm con ở trại nhi đồng\" \ngenerated_text = generate_text(\n    model=gpt_model,\n    tokenizer=tokenizer,\n    prompt=prompt,\n    max_length=max_len,\n    temperature=0.7,\n    top_k=50,\n    device=device\n)\n\nprint(f\"Prompt: {prompt}\")\nprint(f\"Generated text: {generated_text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:53:35.166507Z","iopub.execute_input":"2024-12-04T01:53:35.167328Z","iopub.status.idle":"2024-12-04T01:53:36.011178Z","shell.execute_reply.started":"2024-12-04T01:53:35.167292Z","shell.execute_reply":"2024-12-04T01:53:36.010332Z"}},"outputs":[{"name":"stdout","text":"Prompt: <s> thăm con ở trại nhi đồng\nGenerated text:  thăm con ở trại nhi đồng \\n để rồi ta vẫn đứng ngồi không màn \\n còn đây cũng thật đáng yêu \\n em đây là để nói lời yêu thương \n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"prompt = \"<s> mùa xuân\" \ngenerated_text = generate_text(\n    model=gpt_model,\n    tokenizer=tokenizer,\n    prompt=prompt,\n    max_length=max_len,\n    temperature=0.7,\n    top_k=50,\n    device=device\n)\n\nprint(f\"Prompt: {prompt}\")\nprint(f\"Generated text: {generated_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T01:55:27.808920Z","iopub.execute_input":"2024-12-04T01:55:27.809298Z","iopub.status.idle":"2024-12-04T01:55:28.115249Z","shell.execute_reply.started":"2024-12-04T01:55:27.809270Z","shell.execute_reply":"2024-12-04T01:55:28.114392Z"}},"outputs":[{"name":"stdout","text":"Prompt: <s> mùa xuân\nGenerated text:  mùa xuân về chốn hư hao \\n để anh em chẳng thể nào dám gần \\n nhớ sao duyên dáng em ơi \\n để ta ôm ấp cho mình vì con \n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import random\n\nlst_prompts = []\nfor sentence in sentences[:100]:\n    random_number = random.randint(1, 6)\n    a = sentence.split(' ')\n    s = ''\n    for char in a[:random_number+1]:\n        s = s + char + ' '\n    lst_prompts.append(s[:-1])\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T02:17:13.197446Z","iopub.execute_input":"2024-12-04T02:17:13.198362Z","iopub.status.idle":"2024-12-04T02:17:13.203766Z","shell.execute_reply.started":"2024-12-04T02:17:13.198326Z","shell.execute_reply":"2024-12-04T02:17:13.202818Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"gen_sentences = []\nfor prompt in lst_prompts:\n    generated_text = generate_text(\n        model=gpt_model,\n        tokenizer=tokenizer,\n        prompt=prompt,\n        max_length=max_len,\n        temperature=0.8,\n        top_k=20,\n        device=device\n        )\n    gen_sentences.append(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T02:40:28.980686Z","iopub.execute_input":"2024-12-04T02:40:28.981519Z","iopub.status.idle":"2024-12-04T02:40:55.665187Z","shell.execute_reply.started":"2024-12-04T02:40:28.981484Z","shell.execute_reply":"2024-12-04T02:40:55.664275Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"!pip install einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:22.181897Z","iopub.execute_input":"2024-12-03T01:06:22.182223Z","iopub.status.idle":"2024-12-03T01:06:30.550697Z","shell.execute_reply.started":"2024-12-03T01:06:22.182191Z","shell.execute_reply":"2024-12-03T01:06:30.549744Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from einops import rearrange\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.553157Z","iopub.execute_input":"2024-12-03T01:06:30.553456Z","iopub.status.idle":"2024-12-03T01:06:30.565846Z","shell.execute_reply.started":"2024-12-03T01:06:30.553425Z","shell.execute_reply":"2024-12-03T01:06:30.565015Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class ScaleDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaleDotProductAttention, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, q, k, v, mask=None, e=1e-12):\n        batch_size, head, length, d_tensor = k.size()\n\n        score = torch.einsum(\"bhid,bhjd->bhij\",q,k)\n        score = score/math.sqrt(d_tensor)\n\n        if mask is not None:\n            score = score.masked_fill(mask == 0, -e)\n\n        score = self.softmax(score)\n\n        v = score @ v\n\n        return v, score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.566917Z","iopub.execute_input":"2024-12-03T01:06:30.567560Z","iopub.status.idle":"2024-12-03T01:06:30.577595Z","shell.execute_reply.started":"2024-12-03T01:06:30.567522Z","shell.execute_reply":"2024-12-03T01:06:30.576802Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, n_head):\n        super(MultiHeadAttention, self).__init__()\n        self.n_head = n_head\n        self.attention = ScaleDotProductAttention()\n        self.w_q = nn.Linear(d_model, d_model*n_head)\n        self.w_k = nn.Linear(d_model, d_model*n_head)\n        self.w_v = nn.Linear(d_model, d_model*n_head)\n        self.w_concat = nn.Linear(d_model*n_head, d_model)\n\n    def forward(self, x, mask=None):\n        q, k, v = self.w_q(x), self.w_k(x), self.w_v(x)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_head), (q, k, v))\n\n        out, attention = self.attention(q, k, v, mask=mask)\n\n        # 4. concat and pass to linear layer\n        # out = self.concat(out)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.w_concat(out)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.578604Z","iopub.execute_input":"2024-12-03T01:06:30.578889Z","iopub.status.idle":"2024-12-03T01:06:30.588347Z","shell.execute_reply.started":"2024-12-03T01:06:30.578864Z","shell.execute_reply":"2024-12-03T01:06:30.587543Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class SelfAttentionLstm(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers,n_head):\n        super(SelfAttentionLstm, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.multi_attention = MultiHeadAttention(d_model=input_size,n_head=4)\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n\n    def forward(self, x):\n        x = self.multi_attention(x)\n         \n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(\"cuda\")\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to('cuda')\n\n        out, _ = self.lstm(x, (h0, c0))\n\n        out = out[: ,-1, : ]\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.589477Z","iopub.execute_input":"2024-12-03T01:06:30.589897Z","iopub.status.idle":"2024-12-03T01:06:30.602564Z","shell.execute_reply.started":"2024-12-03T01:06:30.589861Z","shell.execute_reply":"2024-12-03T01:06:30.601900Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def split_list_with_indices(input_list):\n    # Loại bỏ số 0 ở đầu list\n    start_idx = 0\n    while start_idx < len(input_list) and input_list[start_idx] == 0:\n        start_idx += 1\n        \n    # Loại bỏ số 0 ở cuối list\n    end_idx = len(input_list) - 1\n    while end_idx >= 0 and input_list[end_idx] == 0:\n        end_idx -= 1\n        \n    # Tách list và lưu indices\n    result = []\n    indices = []\n    current_start = start_idx\n    \n    i = start_idx\n    while i <= end_idx:\n        if input_list[i] == 266 or i == end_idx:\n            end = i + 1 if input_list[i] == 266 else i + 1\n            sublist = input_list[current_start:end]\n            result.append(sublist)\n            indices.append((current_start, end - 1))\n            current_start = end\n            i = end\n        else:\n            i += 1\n            \n    return indices\n\n\ninput_list = [0, 1027, 378, 640, 3696, 2255, 615, 266, 82, 519, 419, 891, 1021, 371, 581, 2617, 2014, 266, 82, 544, 449, 391, 539, 823, 510, 266, 82, 371, 1353, 445, 1236, 492, 1184, 1837, 528, 225, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nindices = split_list_with_indices(input_list.copy())\nindices\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.603456Z","iopub.execute_input":"2024-12-03T01:06:30.603671Z","iopub.status.idle":"2024-12-03T01:06:30.617329Z","shell.execute_reply.started":"2024-12-03T01:06:30.603650Z","shell.execute_reply":"2024-12-03T01:06:30.616515Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[(1, 7), (8, 17), (18, 25), (26, 36)]"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"def get_idx_two_line(lm_logits):\n    token = torch.argmax(lm_logits, dim= 2)\n    token = token[0].tolist()\n    return split_list_with_indices(token.copy())\n\n    \ndef loss_kho_tho(lm_logits,embedding):\n    lm_logits = torch.unsqueeze(lm_logits,0)\n    pair_list = get_idx_two_line(lm_logits)\n    embedding = torch.unsqueeze(embedding,0)\n    \n    total_lost = 0\n    loss = nn.MSELoss().to(device)\n    for i in range(len(pair_list)-1):\n        one = pair_list[i]\n        two = pair_list[i+1]\n\n        if one == None or two == None:\n          continue\n        \n        # Kiểm tra shape của embedding trước khi đưa vào LSTM\n        embedd_slice_one = embedding[:,one[0]:one[1],:]\n        embedd_slice_two = embedding[:,two[0]:two[1],:]\n        if embedd_slice_one.size(1) == 0 or embedd_slice_two.size(1) == 0:\n            continue\n            \n        embedd_one = head_gpt(embedd_slice_one)\n        embedd_two = head_gpt(embedd_slice_two)\n\n        total_lost += loss(embedd_one,embedd_two)\n    return total_lost     \n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.618371Z","iopub.execute_input":"2024-12-03T01:06:30.618665Z","iopub.status.idle":"2024-12-03T01:06:30.633084Z","shell.execute_reply.started":"2024-12-03T01:06:30.618602Z","shell.execute_reply":"2024-12-03T01:06:30.632185Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"\ndef train_model(model, dataloader, optimizer,tokenizer, device, epochs=3):\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        with tqdm(total=len(dataloader), desc=f\"Epoch {epoch}\") as progress_bar:\n          for batch in dataloader:\n            input_ids = batch['input_ids'].squeeze(1).to(device)            \n            \n            outputs = model(input_ids)[0]\n            embedding = model.wte(input_ids)\n              \n            shift_logits = outputs[..., :-1, :].contiguous()\n            shift_labels = input_ids[..., 1:].contiguous()\n            loss_fct = nn.CrossEntropyLoss()\n              \n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n            loss = loss + sum([loss_kho_tho(shift_logits[i],embedding[i]) for i in range(shift_logits.shape[0])])*100\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n              \n            optimizer.step()\n            losses.append(loss.item())\n            \n            progress_bar.set_postfix({'loss': loss.item()})  # Set 'loss' key-value pair\n            progress_bar.update(1)\n        print(f\"Epoch: {epoch}, Average Loss: {sum(losses) / len(losses)}\")\n        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n        save_checkpoint(checkpoint, filename= \"/kaggle/working/gpt_2_custom_loss_v2.pth.tar\")\n    return losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:06:30.635692Z","iopub.execute_input":"2024-12-03T01:06:30.636000Z","iopub.status.idle":"2024-12-03T01:06:30.650672Z","shell.execute_reply.started":"2024-12-03T01:06:30.635973Z","shell.execute_reply":"2024-12-03T01:06:30.649743Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# import torch.optim as optim\n# from tqdm import tqdm\n\n# head_gpt = SelfAttentionLstm(input_size=768,hidden_size=768, num_layers=2,n_head=4).to('cuda')\n# gpt_losses = train_model(gpt_model, dataloader, optimizer, tokenizer, device, epochs=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:27:13.252268Z","iopub.execute_input":"2024-12-03T06:27:13.252629Z","iopub.status.idle":"2024-12-03T06:27:13.256663Z","shell.execute_reply.started":"2024-12-03T06:27:13.252584Z","shell.execute_reply":"2024-12-03T06:27:13.255669Z"}},"outputs":[],"execution_count":42}]}